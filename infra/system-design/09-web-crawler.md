### 웹 크롤러 설계
* 검색 엔진에서 널리 쓰는 기술로, 웹에 새로 올라오거나 갱신된 콘텐츠를 찾아내는 것이 주된 목적이다.
  * 여기서 콘텐츠는 웹 페이지일 수도 있고, 이미지나 비디오, 또는 PDF 파일일 수도 있다.
* 검색 엔진 인덱싱: 크롤러의 가장 보편적인 용례다. 크롤러는 웹 페이지를 모아 검색 엔진을 위한 로컬 인덱스를 만든다. 일례로 Googlebot은 구글 검색 엔진이 사용하는 웹 크롤러다.
* 웹 아카이빙: 나중에 사용할 목적으로 장기보관하기 위해 웹에서 정보를 모으는 절차를 말한다. 많은 국립 도서관이 크롤러를 돌려 웹 사이트를 아카이빙하고 있다. 대표적으로 미국 국회 도서관, EU 웹 아카이브다 있다.
* 웹 마이닝: 웹의 폭발적 성장세는 데이터 마이닝 업계에 전례 없는 기회다. 웹 마이닝을 통해 인터넷에서 유용한 지식을 도출해 낼 수 있는 것이다. 일례로, 유명 금융 기업들은 크롤러를 사용해 주주 총회 자료나 연차 보고서를 다운 받아 사업 방향을 알아내기도 한다.
* 웹 모니터링: 크롤러를 사용하면 인터넷에서 저작권이나 상표권이 침해되는 사례를 모니터링할 수 있다. 일례로 디지마크 사는 웹 크롤러를 사용해 해적판 저작물을 찾아내서 보고한다.

**문제 이해 및 설계 범위 확정**
1. URL 집합이 입력으로 주어지면, 해당 URL들이 가리키는 모든 웹 페이지를 다운로드한다.
2. 다운받은 웹 페이지에서 URL들을 추출한다.
3. 추출된 URL들은 다운로드할 URL 목록에 추가하고 위의 과정을 처음부터 반복한다.

* 다만 웹 크롤러는 이처럼 단순하게 동작하지 않고 엄청난 규모 확장성을 갖는 웹 크롤러를 설계하는 것은 엄청나게 어려운 작업이다.

**요구 사항**
* 웹 크롤러는 검색 엔진 인덱싱에 쓰인다.
* 10억 개의 웹 페이지를 수집해야한다.
* 수집한 웹 페이지는 5년간 저장해두어야 한다.
* 중복된 콘텐츠를 갖는 페이지는 무시해도 된다.

**개략적 규모 추정**
* 매달 10억 개의 웹 페이지를 다운로드한다.
* QPS = 10억/30일/24시간/3600초 = 대략 400페이지/초
* 최대 QPS = 2 * QPS = 800
* 웹 페이지의 크기 평균은 500k라고 가정
* 10억 페이지 * 500k = 500TB/월
* 1개월치 데이터를 보관하는 데는 500TB, 5년간 보관한다고 가정하면 결국 500TB * 12개월 * 5년 = 30PB의 저장용량이 필요